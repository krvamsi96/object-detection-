{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object detection ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSOUg5_48mEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "8efb912b-e0ad-4488-adf2-7440f6c90b6f"
      },
      "source": [
        "!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tf_slim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow==2.* in /usr/local/lib/python3.6/dist-packages (2.3.0rc2)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (2.2.2)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.*) (2.3.0rc0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.*) (3.1.0)\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjM182f68vs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "e1bb1b47-e13b-406c-bcb6-05f45b28d434"
      },
      "source": [
        "!pip install pycocotools\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (49.1.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (0.29.21)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP_ffYx84lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caTNQe3n9Hp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87683264-9927-44b1-8c00-7ff810eb3402"
      },
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V62k7N3g9W-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "bcd260f6-5f49-449c-ce40-acc2f5fe7575"
      },
      "source": [
        "%%bash \n",
        "cd models/research\n",
        "pip install ."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/models/research\n",
            "Requirement already satisfied: Pillow>=1.0 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: Matplotlib>=2.1 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython>=0.28.1 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.29.21)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->Matplotlib>=2.1->object-detection==0.1) (1.12.0)\n",
            "Building wheels for collected packages: object-detection\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-cp36-none-any.whl size=1304063 sha256=3a9f2fb3dc93d0078ffe4d652e5f5954f2c25871b4f13e279bc3a1fb2f83c1e3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hlerrtln/wheels/94/49/4b/39b051683087a22ef7e80ec52152a27249d1a644ccf4e442ea\n",
            "Successfully built object-detection\n",
            "Installing collected packages: object-detection\n",
            "  Found existing installation: object-detection 0.1\n",
            "    Uninstalling object-detection-0.1:\n",
            "      Successfully uninstalled object-detection-0.1\n",
            "Successfully installed object-detection-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riH6mDf79aRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import six\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import pathlib\n",
        "import json\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4AfjAr_9f3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_utils"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrHr0Bsv9mfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utils_ops.tf = tf.compat.v1\n",
        "tf.gfile = tf.io.gfile"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qBop0fb9q8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model_name):\n",
        "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "  model_file = model_name + '.tar.gz'\n",
        "  model_dir = tf.keras.utils.get_file(\n",
        "    fname=model_name,\n",
        "    origin=base_url + model_file,\n",
        "    untar=True)\n",
        "\n",
        "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
        "  model = tf.saved_model.load(str(model_dir))\n",
        "  model = model.signatures['serving_default']\n",
        "\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDUWekL09uNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_LABELS = 'models/research/object_detection/data/snapshot_serengeti_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZEGgPqs9zPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ea491d82-3de1-47dd-e756-e1bde5e6f4bf"
      },
      "source": [
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images/snapshot_serengeti')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpeg\")))\n",
        "TEST_IMAGE_PATHS"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0038.jpeg'),\n",
              " PosixPath('models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0039.jpeg'),\n",
              " PosixPath('models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0040.jpeg'),\n",
              " PosixPath('models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0041.jpeg')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GptN8TER92i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_json = 'models/research/object_detection/test_images/snapshot_serengeti/context_rcnn_demo_metadata.json'\n",
        "with open(test_data_json, 'r') as f:\n",
        "  test_metadata = json.load(f)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPsMZcaE96Lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "346e4e82-3883-48d1-9a33-77debe2b2e69"
      },
      "source": [
        "image_id_to_datetime = {im['id']:im['date_captured'] for im in test_metadata['images']}\n",
        "image_path_to_id = {im['file_name']: im['id'] \n",
        "                    for im in test_metadata['images']}\n",
        "image_path_to_id"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0038.jpeg': 'S1/E03/E03_R3/S1_E03_R3_PICT0038',\n",
              " 'models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0039.jpeg': 'S1/E03/E03_R3/S1_E03_R3_PICT0039',\n",
              " 'models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0040.jpeg': 'S1/E03/E03_R3/S1_E03_R3_PICT0040',\n",
              " 'models/research/object_detection/test_images/snapshot_serengeti/S1_E03_R3_PICT0041.jpeg': 'S1/E03/E03_R3/S1_E03_R3_PICT0041'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT4dJrAT9_SP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a544c60c-859d-4c9d-9033-76757bcae972"
      },
      "source": [
        "faster_rcnn_model_name = 'faster_rcnn_resnet101_snapshot_serengeti_2020_06_10'\n",
        "faster_rcnn_model = load_model(faster_rcnn_model_name)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H3jaZjN-Dyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4299f2c-185a-4e16-fe81-654443546501"
      },
      "source": [
        "faster_rcnn_model.inputs\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'image_tensor:0' shape=(None, None, None, 3) dtype=uint8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KXW-IUT-LRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "60e7c1f8-5824-4960-be15-da6df4a0f981"
      },
      "source": [
        "faster_rcnn_model.output_dtypes\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'detection_boxes': tf.float32,\n",
              " 'detection_classes': tf.float32,\n",
              " 'detection_features': tf.float32,\n",
              " 'detection_multiclass_scores': tf.float32,\n",
              " 'detection_scores': tf.float32,\n",
              " 'num_detections': tf.float32,\n",
              " 'raw_detection_boxes': tf.float32,\n",
              " 'raw_detection_scores': tf.float32}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s5pOCSe-N4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e5de5afa-dc6e-4164-8b4c-3bfe757ebbc1"
      },
      "source": [
        "faster_rcnn_model.output_shapes\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'detection_boxes': TensorShape([None, 300, 4]),\n",
              " 'detection_classes': TensorShape([None, 300]),\n",
              " 'detection_features': TensorShape([None, None, None, None, None]),\n",
              " 'detection_multiclass_scores': TensorShape([None, 300, 49]),\n",
              " 'detection_scores': TensorShape([None, 300]),\n",
              " 'num_detections': TensorShape([None]),\n",
              " 'raw_detection_boxes': TensorShape([None, 300, 4]),\n",
              " 'raw_detection_scores': TensorShape([None, 300, 49])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHYGyZUi-Qda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  output_dict = model(input_tensor)\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_dets = output_dict.pop('num_detections')\n",
        "  num_detections = int(num_dets)\n",
        "  for key,value in output_dict.items():\n",
        "    output_dict[key] = value[0, :num_detections].numpy() \n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(\n",
        "      np.int64)\n",
        "  return output_dict\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_EvbIT8-mjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_date_captured(date_captured):\n",
        "   embedded_date_captured = []\n",
        "   month_max = 12.0\n",
        "   day_max = 31.0\n",
        "   hour_max = 24.0\n",
        "   minute_max = 60.0\n",
        "   min_year = 1990.0\n",
        "   max_year = 2030.0\n",
        "\n",
        "   year = (date_captured.year-min_year)/float(max_year-min_year)\n",
        "   embedded_date_captured.append(year)\n",
        "\n",
        "   month = (date_captured.month-1)/month_max\n",
        "   embedded_date_captured.append(month)\n",
        "\n",
        "   day = (date_captured.day-1)/day_max\n",
        "   embedded_date_captured.append(day)\n",
        "\n",
        "   hour = date_captured.hour/hour_max\n",
        "   embedded_date_captured.append(hour)\n",
        "\n",
        "   minute = date_captured.minute/minute_max\n",
        "   embedded_date_captured.append(minute)\n",
        "\n",
        "   return np.asarray(embedded_date_captured)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4707Us90-tSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_position_and_size(box):\n",
        "   ymin = box[0]\n",
        "   xmin = box[1]\n",
        "   ymax = box[2]\n",
        "   xmax = box[3]\n",
        "   w = xmax - xmin\n",
        "   h = ymax - ymin\n",
        "   x = xmin + w / 2.0\n",
        "   y = ymin + h / 2.0\n",
        "   return np.asarray([x, y, w, h])\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLBwwZ69-__w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_context_feature_embedding(date_captured, detection_boxes,\n",
        "                                  detection_features, detection_scores):\n",
        "   date_captured = datetime.datetime.strptime(date_captured,'%Y-%m-%d %H:%M:%S')\n",
        "   temporal_embedding = embed_date_captured(date_captured)\n",
        "   embedding = detection_features[0]\n",
        "   pooled_embedding = np.mean(np.mean(embedding, axis=1), axis=0)\n",
        "   box = detection_boxes[0]\n",
        "   position_embedding = embed_position_and_size(box)\n",
        "   bb_embedding = np.concatenate((pooled_embedding, position_embedding))\n",
        "   embedding = np.expand_dims(np.concatenate((bb_embedding,temporal_embedding)),\n",
        "                             axis=0)\n",
        "   score = detection_scores[0]\n",
        "   return embedding, score"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QffYFJHN_JLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_inference(model, image_path, date_captured, resize_image=True):\n",
        "   with open(image_path,'rb') as f:\n",
        "    image = Image.open(f)\n",
        "    if resize_image:\n",
        "      image.thumbnail((640,640),Image.ANTIALIAS)\n",
        "    image_np = np.array(image)\n",
        "\n",
        "  # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(model, image_np)\n",
        "\n",
        "   context_feature, score = get_context_feature_embedding(\n",
        "      date_captured, output_dict['detection_boxes'],\n",
        "      output_dict['detection_features'], output_dict['detection_scores'])\n",
        "   return context_feature, score, output_dict\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgT6WdLb_Qzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_features = []\n",
        "scores = []\n",
        "faster_rcnn_results = {}\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  image_id = image_path_to_id[str(image_path)]\n",
        "  date_captured = image_id_to_datetime[image_id]\n",
        "  context_feature, score, results = run_inference(\n",
        "      faster_rcnn_model, image_path, date_captured)\n",
        "  faster_rcnn_results[image_id] = results\n",
        "  context_features.append(context_feature)\n",
        "  scores.append(score)\n",
        "\n",
        "# Concatenate all extracted context embeddings into a contextual memory bank.\n",
        "context_features_matrix = np.concatenate(context_features, axis=0)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px9rYF7o_hFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e21dfa3-d666-4976-daa6-2599404198b4"
      },
      "source": [
        "context_rcnn_model_name = 'context_rcnn_resnet101_snapshot_serengeti_2020_06_10'\n",
        "context_rcnn_model = load_model(context_rcnn_model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YypTLgj3_nI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_padding_size = 2000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF20nfwy_udU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_rcnn_model.inputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWLyc4oA_xOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_rcnn_model.output_dtypes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p9tAeBn_ztV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_rcnn_model.output_shapes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPIlu3DZ_2DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_context_rcnn_inference_for_single_image(\n",
        "    model, image, context_features, context_padding_size):\n",
        "    image = np.asarray(image)\n",
        "    image_tensor = tf.convert_to_tensor(\n",
        "      image, name='image_tensor')[tf.newaxis,...]\n",
        "    context_features = np.asarray(context_features)\n",
        "    valid_context_size = context_features.shape[0]\n",
        "    valid_context_size_tensor = tf.convert_to_tensor(\n",
        "      valid_context_size, name='valid_context_size')[tf.newaxis,...]\n",
        "    padded_context_features = np.pad(\n",
        "      context_features,\n",
        "      ((0,context_padding_size-valid_context_size),(0,0)), mode='constant')\n",
        "    padded_context_features_tensor = tf.convert_to_tensor(\n",
        "      padded_context_features,\n",
        "      name='context_features',\n",
        "      dtype=tf.float32)[tf.newaxis,...]\n",
        "    output_dict = model(\n",
        "      inputs=image_tensor,\n",
        "      context_features=padded_context_features_tensor,\n",
        "      valid_context_size=valid_context_size_tensor)\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "    num_dets = output_dict.pop('num_detections')\n",
        "    num_detections = int(num_dets)\n",
        "    for key,value in output_dict.items():\n",
        "      output_dict[key] = value[0, :num_detections].numpy() \n",
        "    output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "    return output_dict\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ0XINe2AW-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def show_context_rcnn_inference(\n",
        "    model, image_path, context_features, faster_rcnn_output_dict,\n",
        "    context_padding_size, resize_image=True):\n",
        "  \"\"\"Runs inference over a single input image and visualizes Faster R-CNN vs. \n",
        "  Context R-CNN results.\n",
        "\n",
        "  Args:\n",
        "    model: A tensorflow saved_model object.\n",
        "    image_path: Absolute path to the input image.\n",
        "    context_features: A numpy float32 contextual memory bank of shape \n",
        "      [num_context_examples, 2057]\n",
        "    faster_rcnn_output_dict: The output_dict corresponding to this input image\n",
        "      from the single-frame Faster R-CNN model, which was previously used to\n",
        "      build the memory bank.\n",
        "    context_padding_size: The amount of expected padding in the contextual\n",
        "      memory bank, defined in the Context R-CNN config as \n",
        "      max_num_context_features.\n",
        "    resize_image: Whether to resize the input image before running inference.\n",
        "\n",
        "  Returns:\n",
        "    context_rcnn_image_np: Numpy image array showing Context R-CNN Results.\n",
        "    faster_rcnn_image_np: Numpy image array showing Faster R-CNN Results.\n",
        "  \"\"\"\n",
        "\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  with open(image_path,'rb') as f:\n",
        "    image = Image.open(f)\n",
        "    if resize_image:\n",
        "      image.thumbnail((640,640),Image.ANTIALIAS)\n",
        "    image_np = np.array(image)\n",
        "    image.thumbnail((400,400),Image.ANTIALIAS)\n",
        "    context_rcnn_image_np = np.array(image)\n",
        "    \n",
        "  faster_rcnn_image_np = np.copy(context_rcnn_image_np)\n",
        "\n",
        "  # Actual detection.\n",
        "  output_dict = run_context_rcnn_inference_for_single_image(\n",
        "      model, image_np, context_features, context_padding_size)\n",
        "\n",
        "  # Visualization of the results of a context_rcnn detection.\n",
        "  vis_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      context_rcnn_image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=2)\n",
        "  \n",
        "  # Visualization of the results of a faster_rcnn detection.\n",
        "  vis_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      faster_rcnn_image_np,\n",
        "      faster_rcnn_output_dict['detection_boxes'],\n",
        "      faster_rcnn_output_dict['detection_classes'],\n",
        "      faster_rcnn_output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=2)\n",
        "  return context_rcnn_image_np, faster_rcnn_image_np\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjK0lwHgBNIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [15,10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6XrDExpBQq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  image_id = image_path_to_id[str(image_path)]\n",
        "  faster_rcnn_output_dict = faster_rcnn_results[image_id]\n",
        "  context_rcnn_image, faster_rcnn_image = show_context_rcnn_inference(\n",
        "      context_rcnn_model, image_path, context_features_matrix,\n",
        "      faster_rcnn_output_dict, context_padding_size)\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(faster_rcnn_image)\n",
        "  plt.title('Faster R-CNN')\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(context_rcnn_image)\n",
        "  plt.title('Context R-CNN')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYvYlECiBUVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}